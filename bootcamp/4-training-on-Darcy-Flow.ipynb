{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "012a357f-8533-482c-823d-a4587c49e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import sys\n",
    "from configmypy import ConfigPipeline, YamlConfig, ArgparseConfig\n",
    "from neuralop import get_model\n",
    "from neuralop import Trainer\n",
    "from neuralop.training import setup\n",
    "from neuralop.datasets import load_darcy_pt\n",
    "from neuralop.utils import get_wandb_api_key, count_params\n",
    "from neuralop import LpLoss, H1Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6358b5-78d1-4baf-8928-6bb49b150680",
   "metadata": {},
   "source": [
    "# Loading the configuration\n",
    "\n",
    "You can open the yaml file in config/darcy_config in the same folder as this notebook to inspect the parameters and change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4503f065-4063-4a4f-b00f-06a7c3a88e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the configuration\n",
    "config_name = 'default'\n",
    "pipe = ConfigPipeline([YamlConfig('./darcy_config.yaml', config_name='default', config_folder='./config'),\n",
    "                      ])\n",
    "config = pipe.read_conf()\n",
    "config_name = pipe.steps[-1].config_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d820d-9578-4ad7-80b4-05a5771f1642",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we just setup pytorch and print the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46066d9f-21a3-4aab-b6e1-f7f38e05f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up distributed communication, if using\n",
    "device, is_logger = setup(config)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d599f9-6463-4056-9a4d-72c01d05298e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################\n",
      "#####    CONFIGURATION    #####\n",
      "###############################\n",
      "\n",
      "Steps:\n",
      "------\n",
      " (1) YamlConfig with config_file=./darcy_config.yaml, config_name=default, config_folder=./config\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Configuration:\n",
      "--------------\n",
      "\n",
      "verbose=True\n",
      "arch=tfno2d\n",
      "distributed.use_distributed=False\n",
      "tfno2d.data_channels=3\n",
      "tfno2d.n_modes_height=32\n",
      "tfno2d.n_modes_width=32\n",
      "tfno2d.hidden_channels=64\n",
      "tfno2d.projection_channels=256\n",
      "tfno2d.n_layers=4\n",
      "tfno2d.domain_padding=None\n",
      "tfno2d.domain_padding_mode=one-sided\n",
      "tfno2d.fft_norm=forward\n",
      "tfno2d.norm=group_norm\n",
      "tfno2d.skip=linear\n",
      "tfno2d.implementation=factorized\n",
      "tfno2d.separable=0\n",
      "tfno2d.preactivation=0\n",
      "tfno2d.use_mlp=1\n",
      "tfno2d.mlp.expansion=0.5\n",
      "tfno2d.mlp.dropout=0\n",
      "tfno2d.factorization=None\n",
      "tfno2d.rank=1.0\n",
      "tfno2d.fixed_rank_modes=None\n",
      "tfno2d.dropout=0.0\n",
      "tfno2d.tensor_lasso_penalty=0.0\n",
      "tfno2d.joint_factorization=False\n",
      "opt.n_epochs=150\n",
      "opt.learning_rate=0.005\n",
      "opt.training_loss=h1\n",
      "opt.weight_decay=0.0001\n",
      "opt.amp_autocast=False\n",
      "opt.scheduler_T_max=300\n",
      "opt.scheduler_patience=5\n",
      "opt.scheduler=CosineAnnealingLR\n",
      "opt.step_size=50\n",
      "opt.gamma=0.5\n",
      "data.folder=data/darcy_flow\n",
      "data.batch_size=32\n",
      "data.n_train=3000\n",
      "data.train_resolution=32\n",
      "data.n_tests=[500, 500]\n",
      "data.test_resolutions=[32, 64]\n",
      "data.test_batch_sizes=[32, 32]\n",
      "data.positional_encoding=True\n",
      "data.encode_input=True\n",
      "data.encode_output=False\n",
      "patching.levels=0\n",
      "patching.padding=0\n",
      "patching.stitching=False\n",
      "wandb.log=False\n",
      "wandb.log_test_interval=1\n",
      "\n",
      "###############################\n"
     ]
    }
   ],
   "source": [
    "# Make sure we only print information when needed\n",
    "config.verbose = config.verbose and is_logger\n",
    "\n",
    "#Print config to screen\n",
    "if config.verbose and is_logger:\n",
    "    pipe.log()\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1339c794-3e1c-469b-b0a0-cf968fc1dfa1",
   "metadata": {},
   "source": [
    "# Loading the data \n",
    "\n",
    "We train in one resolution and test in several resolutions to show the zero-shot super-resolution capabilities of neural-operators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3515a85a-40fc-4223-9cdb-8768de37d6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnitGaussianNormalizer init on 3000, reducing over [0, 1, 2, 3], samples of shape [1, 32, 32].\n",
      "   Mean and std of shape torch.Size([1, 1, 1]), eps=1e-05\n",
      "Loading test db at resolution 64 with 500 samples and batch-size=32\n"
     ]
    }
   ],
   "source": [
    "# Loading the Darcy flow training set in 32x32 resolution, test set in 32x32 and 64x64 resolutions\n",
    "train_loader, test_loaders, output_encoder = load_darcy_pt(\n",
    "        config.data.folder, train_resolution=config.data.train_resolution, n_train=config.data.n_train, batch_size=config.data.batch_size, \n",
    "        positional_encoding=config.data.positional_encoding,\n",
    "        test_resolutions=config.data.test_resolutions, n_tests=config.data.n_tests, test_batch_sizes=config.data.test_batch_sizes,\n",
    "        encode_input=config.data.encode_input, encode_output=config.data.encode_output,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8109298a-aca3-45b7-a8de-c5cf4e1c210b",
   "metadata": {},
   "source": [
    "# Creating the model and putting it on the GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db295d23-ab86-4f37-83cc-7af0a8e485ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given argument key='dropout' that is not in TFNO2d's signature.\n",
      "Given argument key='tensor_lasso_penalty' that is not in TFNO2d's signature.\n",
      "Keyword argument out_channels not specified for model TFNO2d, using default=1.\n",
      "Keyword argument lifting_channels not specified for model TFNO2d, using default=256.\n",
      "Keyword argument incremental_n_modes not specified for model TFNO2d, using default=None.\n",
      "Keyword argument non_linearity not specified for model TFNO2d, using default=<built-in function gelu>.\n",
      "Keyword argument decomposition_kwargs not specified for model TFNO2d, using default={}.\n",
      "\n",
      "n_params: 16844673\n"
     ]
    }
   ],
   "source": [
    "model = get_model(config)\n",
    "model = model.to(device)\n",
    "\n",
    "#Log parameter count\n",
    "if is_logger:\n",
    "    n_params = count_params(model)\n",
    "\n",
    "    if config.verbose:\n",
    "        print(f'\\nn_params: {n_params}')\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec85d0a-4db4-4b1f-b599-8c2afc98520a",
   "metadata": {},
   "source": [
    "# Create the optimizer and learning rate scheduler\n",
    "\n",
    "Here, we use an Adam optimizer and a learning rate schedule depending on the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5164537a-267b-4fda-9bcd-257dc3ac4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                lr=config.opt.learning_rate, \n",
    "                                weight_decay=config.opt.weight_decay)\n",
    "\n",
    "if config.opt.scheduler == 'ReduceLROnPlateau':\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=config.opt.gamma, patience=config.opt.scheduler_patience, mode='min')\n",
    "elif config.opt.scheduler == 'CosineAnnealingLR':\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.opt.scheduler_T_max)\n",
    "elif config.opt.scheduler == 'StepLR':\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "                                                step_size=config.opt.step_size,\n",
    "                                                gamma=config.opt.gamma)\n",
    "else:\n",
    "    raise ValueError(f'Got {config.opt.scheduler=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a72eb-965a-4997-89a4-0cdfcbcb0a1a",
   "metadata": {},
   "source": [
    "# Creating the loss\n",
    "\n",
    "We will optimize the Sobolev norm but also evaluate our goal: the l2 relative error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a53d9d-2d06-4d36-9b46-2c7f15f29c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the losses\n",
    "l2loss = LpLoss(d=2, p=2)\n",
    "h1loss = H1Loss(d=2)\n",
    "if config.opt.training_loss == 'l2':\n",
    "    train_loss = l2loss\n",
    "elif config.opt.training_loss == 'h1':\n",
    "    train_loss = h1loss\n",
    "else:\n",
    "    raise ValueError(f'Got training_loss={config.opt.training_loss} but expected one of [\"l2\", \"h1\"]')\n",
    "eval_losses={'h1': h1loss, 'l2': l2loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dad660e-43e9-4f38-91f6-8427b14b8ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### MODEL ###\n",
      " TFNO2d(\n",
      "  (convs): FactorizedSpectralConv2d(\n",
      "    (weight): ModuleList(\n",
      "      (0-7): 8 x ComplexDenseTensor(shape=torch.Size([64, 64, 16, 16]), rank=None)\n",
      "    )\n",
      "  )\n",
      "  (fno_skips): ModuleList(\n",
      "    (0-3): 4 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (mlp): ModuleList(\n",
      "    (0-3): 4 x MLP(\n",
      "      (fcs): ModuleList(\n",
      "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mlp_skips): ModuleList(\n",
      "    (0-3): 4 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (norm): ModuleList(\n",
      "    (0-3): 4 x GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "  )\n",
      "  (lifting): Lifting(\n",
      "    (fc): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (projection): Projection(\n",
      "    (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "\n",
      "### OPTIMIZER ###\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.005\n",
      "    lr: 0.005\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7fc1701caca0>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      " * Train: <neuralop.training.losses.H1Loss object at 0x7fc1701cab80>\n",
      "\n",
      " * Test: {'h1': <neuralop.training.losses.H1Loss object at 0x7fc1701cab80>, 'l2': <neuralop.training.losses.LpLoss object at 0x7fc1701ca250>}\n",
      "\n",
      "### Beginning Training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if config.verbose and is_logger:\n",
    "    print('\\n### MODEL ###\\n', model)\n",
    "    print('\\n### OPTIMIZER ###\\n', optimizer)\n",
    "    print('\\n### SCHEDULER ###\\n', scheduler)\n",
    "    print('\\n### LOSSES ###')\n",
    "    print(f'\\n * Train: {train_loss}')\n",
    "    print(f'\\n * Test: {eval_losses}')\n",
    "    print(f'\\n### Beginning Training...\\n')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5967441-b8bc-4ea8-a4d9-7a5bea384cbf",
   "metadata": {},
   "source": [
    "# Creating the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a19ebfd3-8a2b-42c0-af98-7a1db2dda0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on regular inputs (no multi-grid patching).\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, n_epochs=config.opt.n_epochs,\n",
    "                  device=device,\n",
    "                  mg_patching_levels=config.patching.levels,\n",
    "                  mg_patching_padding=config.patching.padding,\n",
    "                  mg_patching_stitching=config.patching.stitching,\n",
    "                  wandb_log=config.wandb.log,\n",
    "                  log_test_interval=config.wandb.log_test_interval,\n",
    "                  log_output=False,\n",
    "                  use_distributed=config.distributed.use_distributed,\n",
    "                  verbose=config.verbose and is_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a3727-313d-4219-8f8f-0cec58d74b00",
   "metadata": {},
   "source": [
    "# Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d6e3298-99ee-4371-8bad-60e6aac03d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 3000 samples\n",
      "Testing on [500, 500] samples         on resolutions [32, 64].\n",
      "[0] time=56.46, avg_loss=8.5375, train_err=0.4269, 32_h1=0.2190, 32_l2=0.1626, 64_h1=0.2666, 64_l2=0.1622\n",
      "[1] time=56.42, avg_loss=3.7705, train_err=0.1885, 32_h1=0.1750, 32_l2=0.1271, 64_h1=0.2330, 64_l2=0.1358\n",
      "[2] time=55.41, avg_loss=2.9544, train_err=0.1477, 32_h1=0.1425, 32_l2=0.1028, 64_h1=0.2113, 64_l2=0.1128\n",
      "[3] time=55.57, avg_loss=2.7224, train_err=0.1361, 32_h1=0.1216, 32_l2=0.0812, 64_h1=0.1970, 64_l2=0.0933\n",
      "[4] time=57.34, avg_loss=2.3618, train_err=0.1181, 32_h1=0.1157, 32_l2=0.0746, 64_h1=0.1939, 64_l2=0.0845\n",
      "[5] time=54.25, avg_loss=2.0741, train_err=0.1037, 32_h1=0.1086, 32_l2=0.0681, 64_h1=0.1861, 64_l2=0.0804\n",
      "[6] time=56.55, avg_loss=2.0932, train_err=0.1047, 32_h1=0.1025, 32_l2=0.0614, 64_h1=0.1808, 64_l2=0.0704\n"
     ]
    }
   ],
   "source": [
    "trainer.train(train_loader, test_loaders,\n",
    "              output_encoder,\n",
    "              model, \n",
    "              optimizer,\n",
    "              scheduler, \n",
    "              regularizer=False, \n",
    "              training_loss=train_loss,\n",
    "              eval_losses=eval_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b20be56-d200-44dc-b97b-fca021e353c8",
   "metadata": {},
   "source": [
    "# Follow-up questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a67e1d5-4b9a-4be3-bff4-fb2a6b152f9c",
   "metadata": {},
   "source": [
    "You can now play with the configuration and see how the performance is impacted.\n",
    "\n",
    "Which parameters do you think will most influence performance? \n",
    "Learning rate? Learning schedule? hidden_channels? Number of training samples? \n",
    "\n",
    "Does your intuition match the results you are getting?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
